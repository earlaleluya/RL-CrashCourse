{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8682f857",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Crash Course\n",
    "\n",
    "This notebook serves as a supplementary resource for teaching Reinforcement Learning. It provides a concise overview of how RL works, illustrated through the classic CartPole balancing problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9846e6",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "`Environment` receives the agentâ€™s actions, changes its state, and provides observations back to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.python.environment import Environment\n",
    "\n",
    "env = Environment()\n",
    "env.draw_plot(x=2, theta=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a8ead",
   "metadata": {},
   "source": [
    "Task 1: Try replacing the value of x and theta\n",
    "\n",
    "`env.draw_plot(x=-2, theta=0.52)`\n",
    "\n",
    "then,\n",
    "\n",
    "`env.draw_plot(x=2, theta=-0.52)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278fc9d6",
   "metadata": {},
   "source": [
    "## Defining the State\n",
    "`State` a state is a snapshot of the environment at a given time. It contains the information the agent needs to make a decision about what action to take next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.python.state import State\n",
    "\n",
    "state = State(n_states=256)    # n_states = n_bins**4\n",
    "state.x = 2.0\n",
    "state.x_dot = 0.2\n",
    "state.theta = 0.12\n",
    "state.theta_dot = 0.5\n",
    "print(state.compute_state_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd70af8d",
   "metadata": {},
   "source": [
    "Task 2: Try replacing the values of any of the state parameters (x, x_dot, theta, theta_dot)\n",
    "\n",
    "Example:<br>\n",
    "`state.x = 0`<br>\n",
    "`state.x_dot = 0`<br>\n",
    "`state.theta = 0`<br>\n",
    "`state.theta_dot = 0`\n",
    "\n",
    "Question: What do we represent the state with a singular scalar integer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecb9746",
   "metadata": {},
   "source": [
    "## Defining the Reward\n",
    "`Reward` is a signal from the environment that tells the agent how good or bad its last action was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.python.reward import Reward \n",
    "\n",
    "reward = Reward()\n",
    "state.x = 0\n",
    "state.x_dot = 0\n",
    "state.theta = 0\n",
    "state.theta_dot = 0\n",
    "state_value = state.compute_state_value()\n",
    "print(reward.compute_reward(state_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1bdde",
   "metadata": {},
   "source": [
    "Task 3: Try replacing the state parameter values. Note: `1.0` means no-fail, while `0.0` means fail.\n",
    "\n",
    "Example:<br>\n",
    "`state.x = 5.1`<br>\n",
    "`state.x_dot = 0`<br>\n",
    "`state.theta = 0`<br>\n",
    "`state.theta_dot = 0`\n",
    "\n",
    "Question: What if we modify the reward system where `0.0` for every time step the pole remains balanced, while `-1.0` when the pole falls?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64db903",
   "metadata": {},
   "source": [
    "## Defining the Action\n",
    "`Action` is a choice the agent makes to interact with the environment at a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.python.action import Action\n",
    "\n",
    "action = Action()\n",
    "print(action.explore())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b71be",
   "metadata": {},
   "source": [
    "Task: Try rerunning the cell above multiple times to witness the randomly decided actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce0a980",
   "metadata": {},
   "source": [
    "## Constructing an Agent\n",
    "`Agent` is the learner or decision-maker that interacts with the environment to achieve a goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.python.agent import Agent\n",
    "\n",
    "agent_q = Agent(algorithm='Q', n_states=256, n_actions=2)\n",
    "print(action.exploit(agent_q, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a6f9a",
   "metadata": {},
   "source": [
    "Let us first try using `Q-learning` algorithm. We can visualize the initial Q matrix where all values are zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_q.algorithm.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f94eb",
   "metadata": {},
   "source": [
    "Next, let us try using `SARSA` algorithm for the agent. We can visualize its initial Q matrix by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe92de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.python.agent import Agent\n",
    "\n",
    "agent_sarsa = Agent(algorithm='SARSA', n_states=256, n_actions=2)\n",
    "print(agent_sarsa.algorithm.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17c4e7",
   "metadata": {},
   "source": [
    "## Training\n",
    "We will try to train the agent using both algorithm. We can do this by executing the `demo.py`. \n",
    "<br><br>\n",
    "For training with `Q-learning`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python demo.py --mode train --algorithm Q --n_episodes 2000 --n_steps 2500 --n_states 256 --save_path examples/python/Q.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aeb198",
   "metadata": {},
   "source": [
    "For training with `SARSA`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ddbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python demo.py --mode train --algorithm SARSA --n_episodes 2000 --n_steps 2500 --n_states 256 --save_path examples/python/SARSA.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7a5d4",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af1a04",
   "metadata": {},
   "source": [
    "Lastly, we can test your trained agent.\n",
    "<br><br>\n",
    "For testing with `Q-learning`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python demo.py --mode test --algorithm Q --data_path examples/python/Q.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f0b37",
   "metadata": {},
   "source": [
    "For testing with `SARSA`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python demo.py --mode test --algorithm SARSA --data_path examples/python/SARSA.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e2e085",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
